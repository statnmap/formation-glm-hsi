[
["1-preface.html", "Formation aux GLM et aux modèles de distribution d’espèces 1 Préface", " Formation aux GLM et aux modèles de distribution d’espèces Sébastien Rochette 27 mai, 2019 1 Préface La version d’origine de cette formation a été créée par Olivier Le Pape et Étienne Rivot à Agrocampus Ouest (Rennes, France). Depuis mon doctorat dans leur équipe, je mets à jour constamment cette formation au gré de ma recherche et de l’évolution du logiciel R. # Generated with R and rmarkdown: Roadmap version - Teacher "],
["2-presentation-de-letude.html", "2 Présentation de l’étude 2.1 Contexte 2.2 Objectifs 2.3 Données 2.4 Covariables 2.5 Ajuster un modèle de distribution d’espèces 2.6 Exploration des données", " 2 Présentation de l’étude Le contexte et les objectifs de votre étude définissent le type de modélisation que vous allez mettre en place sur votre jeu de données. Ici, nous utilisons les modèles linéaires généralisés pour produire une carte de distribution moyenne de la nourricerie de soles communes de la baie de Vilaine. 2.1 Contexte Les zones côtières et les estuaires sont des habitats halieutiques essentiels Zones à forte production Nourriceries Zones restreintes avec de fortes densités (Fig. 2.1) Figure 2.1: Plaice box (Rijnsdorp et al.) Pression anthropique élevée Perte de surface disponibles (Fig. 2.2a) Qualité des habitats alterée (Fig. 2.2b) Impact sur le renouvellement des populations Jeune stades = Gouleau d’étranglement La taille et la qualité des nourriceries côtières influent sur la production de juvéniles Figure 2.2: (a) L’estuaire de la Seine. (b) Niveau de contamination chimique le long des côtes françaises (Ifremer, 2011) 2.2 Objectifs Déterminer les facteurs ayant une influence sur la distribution des poissons plats (Solea solea) en Baie de Vilaine et cartographier la distribution moyenne des densités. Cartographier les habitats potentiels nécessite: Connaissance des habitats de juvéniles Campagnes d’échantillonnage dans la zone d’étude Connaissance des covariables environnementales ayant potentiellement de l’influence Cartes exhaustives des covariables environnementales Une approche statistique en deux étapes Modèle statistique reliant les densités aux covariables Prédire les habitats potentiels 2.3 Données Campagne standardisée de chalut à perche dans la baie de Vilaine (Fig. 2.3) 1984 – 2010 En autumne Juvéniles de l’année (Âge 0) Nb individus / 1000m2 Figure 2.3: (a) L’estuaire de la Vilaine. (b) Chalut à perche. (c) Situation des stations d’échantillonnage. 2.4 Covariables Bathymétrie (Fig. 2.4a) MNT à 1000m de résolution Projection Mercator Structure sédimentainre (Fig. 2.4b) Fichier shape de polygones Coordonnées géographiques Zones biologiques (Fig. 2.4c) Combinaison bathymétrie, sédiment, habitat Fichier shape de polygones Coordonnées géographiques Figure 2.4: Covariables en baie de Vilaine. (a) Structure sédimentaire, (b) Bathymétrie et (c) Zones biologiques. 2.5 Ajuster un modèle de distribution d’espèces Croiser les données avec les cartes de covariables Utiliser un modèle linéaire Utiliser les cartes des covariables pour la prédiction (Fig. 2.5) Une prédiction pour chaque cellule d’un raster Figure 2.5: Procédure pour un modèle de distribution d’espèce 2.6 Exploration des données Prenez le temps d’explorer vos données avant toutes analyses Explorer les données et les covariables Explorer le plan d’échantillonnage Explorer les liens potentiels entre les densités et les covariables Explorer les futurs paramètres de modélisation (interactions, distributions) Souvenez-vous toujours des objectifs de votre étude ! Question : Que recherchons-nous dans cette exploration ? "],
["3-preparation.html", "3 Préparation 3.1 Structure des dossiers 3.2 Débutons avec R", " 3 Préparation 3.1 Structure des dossiers Il convient de toujours conserver les fichiers originaux : les reprojections entraînent toujours quelques pertes, mieux vaut revenir aux originaux lorsque c’est possible. L’arborescence de votre dossier de travail est la suivante : 01_Original_data DEPARTEMENTS Sedim_GDG_wgs84 bathy_GDG_1000_merc (and co) Data_Vilaine_solea.csv 02_Outputs 03_Figures 04_Functions 3.2 Débutons avec R Créer un projet Rstudio dans le dossier principal de travail. Ouvrez le script R : “Classic_PresAbs_Positive_HSI_Teacher.R” Lister les différents sous-dossier de travail au début de votre script R # Define working directories --------------------------------------------------- WD &lt;- here() # Folder of original files origWD &lt;- here(&quot;01_Original_data&quot;) # Folder for outputs saveWD &lt;- here(&quot;02_Outputs&quot;) # Folder where to save outputs from R figWD &lt;- here(&quot;03_Figures&quot;) # Folder where complementary functions are stored funcWD &lt;- here(&quot;04_Functions&quot;) "],
["4-modele-delta.html", "4 Modèle Delta 4.1 Étapes 4.2 Sous-modèle sur données positives 4.3 Sous-modèle Binomial 4.4 Couplage des deux sous-modèles 4.5 Conclusion", " 4 Modèle Delta 4.1 Étapes Le modèle sur les données complètes n’était pas satisfaisant. Pour mieux prendre en compte (1) les données d’absences et (2) les fortes valeurs de densités, nous allons utiliser un approche Delta. Le modèle Delta sépare les données en deux sous-groupes, un pour la présence-absence, l’autre pour les densités lorsqu’il y a présence. Construction d’un modèle de présence / absence Distribution binomiale Prédiction de probabilités de présence Construction d’un modèle sur données positives Distribution à définir Prédiction des densités lorsqu’il y a présence Puisque les modèles sont ajustés séparément, ils peuvent inclure des covariables différentes. L’approche Delta couple les deux sous-modèles Sous-modèle Binomial : \\(p_{0/1}\\) Sous-modèle positif : \\(Dens_{+}\\) Couplage: \\(Density = p_{0/1} \\cdot Dens_{+}\\) 4.2 Sous-modèle sur données positives 4.2.1 Étapes La procédure à adopter avec le sous-groupe de données est la même qu’avec le jeu de données complet. Créer un sous-jeu de données contenant uniquement les observations positives Explorer ce nouveau jeu de données Explorer les effets potentiels des covariables Explorer les potentielles loi de distributions Tester les interactions Choisir le meilleur modèle 4.2.2 Exploration L’utilisation d’une transformation log des données montre des distributions proche d’une loi Gaussienne lorsqu’on sépare par covariables (Fig. 4.1). Figure 4.1: Observations des densités log-transformées par rapport aux covariables Bathymétrie et Sédiments 4.2.3 Sélection du meilleur modèle L’exploration des données et le critère d’Akaike conduisent à choisir une distribution log-normale pour les données. En travaillant avec des covariables continues (comme ici la bathymétrie), il est possible que la relation avec les observations ne soit pas linéaire. Dans le cadre des GLM, vous pouvez utiliser des polynômes de différents degrés pour intégrer la non-linéarité de la réponse. L’analyse des résidus sur le modèle sélectionné montre des résultats plutôt satisfaisant (Fig. 4.2). Figure 4.2: Figures de diagnostic du meilleur modèle sélectionné 4.2.4 Validation du modèle En utilisant les différents indices présenté précédemment, vous choisissez le modèle qui s’ajuste le mieux à vos données. C’est donc le meilleur modèle pour décrire vos observations. Dans notre cas, nous souhaitons aussi utiliser ce modèle pour faire de la prédiction, ce qui nécessite de sélectionner un modèle qui donne de bonnes prédictions sur des données non-utilisées pour l’ajustement du modèle. Pour cela, nous pouvons utiliser la validation croisée : Ajuster un modèle sur 90% des données par exemple Utiliser le modèle ajusté pour faire une prédiction pour les 10% restants Comparer les prédictions aux observations Choisir le modèle ayant le meilleur indice de comparaison Vous pourriez utiliser le coefficient de corrélation entre les prédictions et les données de validation comme un indice de qualité d’ajustement pour sélectionner le meilleur modèle. Cependant, l’erreur quadratique moyenne (MSE = Mean Squared Error) voire sa racine (RMSE = Root MSE) est l’indice recommandé. Il mesure la distance moyenne d’une observation à sa prédiction. La validation croisée en k-parties est une des meilleurs façons de faire de la validation croisée. La validation croisée en \\(k = 10\\) parties est l’une des plus utilisées. Elle divise le jeu de données en 10 parts égales et répète la validation croisée pour chacune des 10 sous-parties utilisées comme jeu de données de validation (Fig. 4.3). Dans notre cas, la validation croisée est un peu délicate car nous avons des répétitions d’observations sur chaque station échantillonnée plusieurs années de suite. Si la variabilité inter-annuelle est faible, toutes les données d’une même station seront égales et donc les données de validation seront similaires aux données d’ajustement, rendant la validation croisée peu intéressante. Soyez donc prudents avec la validation croisée lorsqu’il y a suspicion de forte corrélation de vos données ! Pour passer outre ce problème de corrélation, il faut sélectionner les données de validation de manière judicieuse… Le modèle sélectionné sur la base de l’AIC est-il toujours le meilleur modèle avec le RMSE ? Figure 4.3: Illustration de la sélection de jeux de données de validation pour une validation croisée en 10 parties 4.3 Sous-modèle Binomial 4.3.1 Étapes La procédure à adopter avec le sous-groupe de données est la même qu’avec le jeu de données complet. Créer les observations de présence-absences à partir du jeu de données Explorer ce nouveau jeu de données (Fig. 4.4) Utiliser une distribution binomiale Tester les covariables, les interactions, les fonctions de lien, les critères de qualité Choisir le meilleur modèle 4.3.2 Exploration Figure 4.4: Répartition des observations en fonction de la bathymetry et des sédiments 4.3.3 Ajuster un modèle binomial avec une fonction de lien Le choix de la distribution pour un modèle de présence-absence est simple, c’est un modèle binomial. Cepedant, un modèle est généralement ajusté sur la base de résidus Gaussiens. Pour ajuster un modèle binomial, les données doivent être transformées de telle sorte qu’on puisse ajuster un modèle linéaire Gaussien classique dessus. Pour cela, nous utilisons une fonction de lien. La fonction de lien classique d’un modèle binomial est la fonction logit, mais ce n’est pas la seule. Vous pouvez tester cloglog, probit ou cauchit. La fonction logit est la suivante (Fig. 4.5): \\[logit(p) = log \\left( p \\over {1 - p} \\right)\\] Cette fonction tranforme les valeurs dans l’intervalle [0;1] en valeurs dans [-Inf;Inf], de telle sorte que le modèle ajusté soit : \\[logit(p) = Covariate1 + Covariate2 + N(0, \\sigma)\\] où \\(p\\) est la probabilité de présence que l’on peut retrouver après ajustement en utilisant la fonction inverse (\\(logit^{-1}\\)). L’analyse des résidus d’un modèle binomial est aussi à faire, même si on n’a pas vraiment le choix du modèle. Les sorties graphiques sont particulières à analyser (Fig. 4.6). Figure 4.5: Différentes fonctions de lien possibles pour un modèle binomial Figure 4.6: Analyse des résidus d’un modèle binomial 4.3.4 Qualité d’ajustement d’un modèle binomial Une mesure couramment utilisée pour la qualité d’ajustement d’un modèle binomial est “l’aire sous la courbe” (AUC : Area Under the Curve). Un objectif des modèles binomiaux étant de prédire un succès ou un échec, et non pas seulement une probabilité de succès, on peut vouloir définir un seuil (intuitivement 0.5 par exemple) qui transforme la probabilité de présence en présence ou absence. L’AUC est en quelque sorte une probabilité de classer correctement les présences et absences. Une définition plus complète serait : La probabililité moyenne pour qu’une observation=1 et une observation=0 choisies de manière aléatoire dans le jeu de données montrent une probabilité de présence prédite supérieure pour l’observation=1 par rapport à celle de l’observation=0 Ainsi, \\(AUC = 1\\) montrerait un modèle “parfait”, mais \\(AUC = 0.5\\) montrerait un modèle plus mauvais que le hasard. L’AUC s’appelle ainsi parce qu’elle est calculée à partir d’une courbe “ROC” (Receiving Operating Characteristic) qui compare le taux de vrais positifs (sensitivity) au taux de faux positifs (specificity) pour différentes valeurs de seuil (Fig. 4.7). Figure 4.7: (a) Prédiction vs Observations. (b) Courbe ROC d’un modèle binomial 4.3.5 Choix du meilleur seuil La validation croisée en k-parties est une des meilleurs façons de faire de la validation croisée. La validation croisée en \\(k = 10\\) parties est l’une des plus utilisées. Elle divise le jeu de données en 10 parts égales et répète la validation croisée pour chacune des 10 sous-parties utilisées comme jeu de données de validation (Fig. 4.3). Dans notre cas, la validation croisée est un peu délicate car nous avons des répétitions d’observations sur chaque station échantillonnée plusieurs années de suite. Si la variabilité inter-annuelle est faible, toutes les données d’une même station seront égales et donc les données de validation seront similaires aux données d’ajustement, rendant la validation croisée peu intéressante. Soyez donc prudents avec la validation croisée lorsqu’il y a suspicion de forte corrélation de vos données ! Pour passer outre ce problème de corrélation, il faut sélectionner les données de validation de manière judicieuse… Le modèle sélectionné sur la base de l’AIC est-il toujours le meilleur modèle avec l’AUC sur les données de validation ? Figure 4.8: Illustration de la sélection de jeux de données de validation pour une validation croisée en 10 parties 4.4 Couplage des deux sous-modèles 4.4.1 L’approche Delta L’approche Delta est la méthode pour coupler les deux sous-modèles. En réalité, les deux modèles sont simplement multipliés l’un à l’autre. Sur l’exemple simple de la figure 4.9, la densité moyenne est \\(3.12\\), ce qui est simplement la somme de toutes les densités observées (\\(sum = 50\\)) divisée par le nombre d’observations (\\(nb = 16\\)). Cependant, la moyenne peut être calculée différemment. Dans cet exemple, on observe \\(9\\) présences parmi les \\(16\\) échantillons. La probabilité de présence est donc \\(0.56\\). Si on ne regarde que les valeurs positives, la moyenne de ces valeurs positives est \\(5.56\\). Ainsi, la moyenne des densités sur la zone est \\(0.56 \\times 5.56 = 3.12\\). Le couplage des deux sous-modèles c’est : Sous-modèle binomial: \\(p_{0/1} \\sim Bathymetry + Sediment\\) Sous-modèle positif: \\(Dens_{+} \\sim Bathymetry\\) Si log-transformation: \\(Dens_{+} = exp(log(Y_{+})) \\times exp(-0.5 \\cdot \\sigma^{2} \\cdot log(Y_{+}))\\) Couplage: \\(Density = p_{0/1} \\cdot Dens_{+}\\) Figure 4.9: Exemple de densités observées 4.4.2 Comparaison avec les observations La prédiction moyenne calculée avec le modèle Delta peut être comparée aux observations (Fig. 4.10). Cependant, vous devez vous souvenir de la distribution originale des données (Fig. 4.11). Un modèle Delta c’est l’estimation de la moyenne des observations en fonction d’une combinaison de covariables. Cette moyenne ne représente pas la variabilité des observations, ni la proportion d’absences, ni les quelques fortes valeurs de densités observées. Pour prédire parfaitement la variabilité des observations, il faudrait intégrer les covariables evironnementales qui l’explique. Le modèle développé ici n’intègre pas la totalité des effets environnementaux, mais il donne une bonne idée des densités moyennes observées en fonction des covariables sélectionnées. Figure 4.10: Prédictions comparées aux observations. La figure de droite est un zoom de celle de gauche. Figure 4.11: (a) Forme de distribution issue d’un modèle Delta. Représentation de l’incertitude (b) moyenne et 2*écart-type, (c) médiane et quantiles 10% - 90%. 4.4.3 Prédictions Le modèle peut être utilisé pour prédire sur un nouveau jeu de données (Fig. 4.12). Avec ces modèles, il est très simple de calculer des prédictions pour n’importe quelle valeur des covariables, cependant, il est important de ne jamais prédire en dehors de l’étendue des valeurs de covariables observées (Fig. 4.13). Le nombre d’observation dans les différentes combinaisons de covariables a aussi de l’importance, c’est pourquoi l’exploration des données est nécessaire. De plus, en utilisant une transformation log des données, une petite différence de prédiction peut devenir très élevée dans l’échelle d’origine des données. Figure 4.12: Prédiction des moyennes pour les différentes combinaisons des covariables Figure 4.13: Illustration de modèles ajustés sur les densités log-transformées et leurs extrapolations 4.5 Conclusion Séparation des absences et des données positives Construction d’un modèle adapté aux données binomiales Construction d’un modèle adapté aux données positives à large distribution Qualité d’ajustement sur les sous-modèles meilleure que sur les données brutes Couplage des deux sous-modèles Interprétation biologique sensée Question de l’estimation de l’incertitude Prédictions : Indice de qualité des habitats "],
["5-modele-dhabitat.html", "5 Modèle d’habitat 5.1 Étapes 5.2 Préparation des données 5.3 Prédictions", " 5 Modèle d’habitat 5.1 Étapes La réalisation d’une carte de distribution d’espèce (Fig. 5.1) nécessite : Un modèle d’habitat potentiel Indice de qualité d’habitat Modèle: \\(Density \\sim Bathymetry + Sediment\\) Les cartes complètes des covariables Une carte des prédictions du modèle Figure 5.1: Procédure pour cartographier une distribution d’espèce Une manière simple de réaliser la carte des prédictions est de créer un raster qui rassemble l’information de toutes les cartes des covariables nécessaires, puis d’utiliser le modèle pour prédire dans chaque cellule du raster (Fig. 5.2). Figure 5.2: Prédiction de densité pour chaque cellule d’une carte au format raster 5.2 Préparation des données Pour pouvoir faire les prédictions dans le raster, ses couches doivent avoir le même nom que les colonnes du jeu de données. De plus, un raster est une matrice de valeurs numériques, ce qui oblige à convertir les covariables en classe en valeurs numériques, de telle sorte que les niveau de facteur du raster correspondent à ceux des données, et donc existent dans les modèles. Soyez prudents avec la conversion vers des valeurs numériques, les données doivent rester au format facteur et ne doivent pas être utilisées comme valeurs numériques dans les modèles ! 5.3 Prédictions La fonction predict a une méthode pour pouvoir être utilisée directement sur un objet Raster (Fig. 5.3). Figure 5.3: Prédiction des densités de soles en baie de Vilaine. Échelle de couleur en fonction des quantiles des données originales (10%, 50%, 75%, 95%). "],
["6-conclusion-1.html", "6 Conclusion 6.1 Modélisation 6.2 Modèle Delta 6.3 Modèles de distribution d’espèces 6.4 Exemples d’applications", " 6 Conclusion 6.1 Modélisation Importance de l’exploration des données Validation des données Loi de distribution Options pour les modèles Étude des modèles : une approche itérative Choix de la loi de distribution Choix des combinaisons de covariables Vérification des hypothèses Analyse des résidus Critères de qualité d’ajustement Gardez toujours vos objectifs en tête ! 6.2 Modèle Delta Utilité d’un modèle Delta Les données brutes ne peuvent être modélisées Sens biologique Présence &amp; densités: pas forcément les mêmes covariables Utilisation d’un modèle Delta Prédictions utiles Les paramètres des sous-modèles n’ont pas de sens dans le modèle couplé Alternatives Autres modèles zero-inflated ? Distribution tweedie ? GAM (Attention aux données nécessaires et à l’interprétation) Régression quantile (habitat préférentiels) Random forest (Attention à votre question) 6.3 Modèles de distribution d’espèces Outils utiles pour les connaissances biologiques et pour la gestion Modèle Delta approprié pour les données d’espèces marines Fiable si utilisé avec précaution Ce ne sont que des corrélations… Cette formation est un exemple simple D’autres perspectives Ajouter une covariable biotique Approche multi-spécifique Pressions anthropiques ? D’autres outils existent Votre question détermine l’outil à utiliser 6.4 Exemples d’applications Effet de la destruction d’habitat sur la biomasse de juvéniles (Fig. 6.1) Cas de la sole commune dans l’estuaire de Seine Comparaison entre 1850 et 2004 Perte en surface : 33% Perte en biomasse : 42% Figure 6.1: Modélisation des effets de la destruction d’habitats sur la biomasse de sole en Seine. Rochette, S., Rivot, E., Morin, J., Mackinson, S., Riou, P., Le Pape, O. (2010). Effect of nursery habitat degradation on flatfish population renewal. Application to Solea solea in the Eastern Channel (Western Europe). Journal of sea Research, 64 : 34-44. Estimation de stock pour la gestion (Fig. 6.2) Cas des laminaires du Parc marin d’Iroise Estimation des biomasses Validation avec les pêcheurs Proposition de gestion spatialisée Figure 6.2: Estimation spatialisée des biomasses de laminaires dans le parc marin d’Iroise pour la gestion de la ressource. Bajjouk T., Rochette S., Ehrhold A., Laurans M., Le Niliot P. (2015). Multi-approach mapping to help spatial planning and management of the kelp species L. digitata and L. hyporborea: Case study of the Molène archipelago, Brittany. Journal of Sea Research. "]
]
